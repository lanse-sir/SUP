model_configs:
  # input file .
  input_sent: ../data/para5m/train/sentence_4-30_bpe.txt
  input_parser: ../data/para5m/train/template_4-30.txt
  input_ref: ../data/para5m/train/sentence_4-30_bpe.txt
  dev_sent: ../data/para50w/dev/dev_src_sent.txt
  dev_parser: ../data/para50w/dev/dev_tgt_template.txt
  dev_ref: ../data/para50w/dev/dev_tgt_sent.txt
  template: ../data/para50w/template-10.txt
  vocab: ../data/para50w/word_vocab_2
  parser_vocab: ../data/para50w/parse_vocab
  pretrain_emb: ../data/glove.42B.300d.txt
  sim_emb: ../data/glove.42B.300d.txt
  word_occur: ../data/word_occurence.pkl
  # training setup
  gumbel: true
  sample: true
  cat_add: other
  fix_word_emb: false
  seed: 0
  gpu: 0
  tree_gru: true
  batch_size: 64
  lr: 0.0001
  # lr_decay: 0.9
  noise: false
  sent_max_time_step: &smts 35
  dec_ratio: 3.2
  parse_max_time_step: &tmts 200
  decode_max_time_step: *smts
  cuda: true
  clip: 5.0
  eval_bs: 10
  greedy: true
  linear_warmup: false
  warm_up: 5000
  epoch: 5
  patience: 15
  bow_ratio: 1.0
  # seq2seq model parameter
  embed_size: &ves 300
  hidden_size: &vhs 300
  num_layers: &vnl 1
  rnn_drop: &vrd 0.0
  rnn_type: gru
  enc_embed_dim: *ves
  tree_embed_dim: 300
  enc_hidden_dim: *vhs
  enc_num_layers: *vnl
  bidirectional: true
  mapper_type: mapping
  dec_embed_dim: *ves
  dec_hidden_dim: *vhs
  dec_num_layers: *vnl
  # vae parameter
  latent_size: 300
  sample_size: 5
  share_embed: true
  enc_ed: 0.0
  enc_rd: 0.0
  dec_ed: *vrd
  dec_rd: *vrd
  unk_rate: 0.25
  k: 0.0025
  x0: 5000
  anneal_function: fixed
  unk_schedule: fixed
  src_wd: false
  tgt_wd: true
  peak_anneal: false
  init_step_kl_weight: ~
  stop_clip_kl: 0.3
  kl_sem: 0.1
  #  aux_weight: ~
  bow_factor: 0.0
  mul_sem: 5.0
  content_factor: 0.0
  # cycle reconstruction loss.
  cycle_factor: 1.0
  # wm_factor: 1.0
  cls_factor: 2.5
  # metrics.
  metric: acc
  ref_factor: 8
  # pretrained classifer model.
  style_transfer: true
  style_transfer_file: classifer/para5m/checkpoint/model_1
  report: ~