model_configs:
  # input file .
  input_sent: ../data/para5m/train/sentence_4-30_bpe.txt
  input_parser: ../data/para5m/train/parse_4-30.txt
  input_ref: ../data/para5m/train/sentence_4-30_bpe.txt
  dev_sent: ../data/para50w/dev/dev_src_sent.txt
  dev_parser: ../data/para50w/dev/dev_tgt_parse.txt
  dev_ref: ../data/para50w/dev/dev_tgt_sent.txt
  vocab: ../data/para5m/train/vocab/vocab_0.txt
  parser_vocab: ../data/para50w/parse_vocab
  pretrain_emb: ../data/glove.42B.300d.txt
  word_occur: ../data/word_occurence.pkl
  # bpe file
  bpe: true
  bpe_codes_path: ../data/para5m/train/vocab/code_file_32k.txt
  bpe_vocab_path: ../data/para5m/train/vocab/vocab_32k.txt
  bpe_vocab_thresh: 50
  # pretrain_emb: ../data/GoogleNews-vectors-negative300.bin
  # training setup
  cat_add: other
  fix_word_emb: false
  seed: 0
  gpu: 0
  tree_gru: true
  smoothing: false
  bow_ratio: 1.0
  batch_size: 128
  lr: 0.0005
  noise: false
  sent_max_time_step: &smts 35
  dec_ratio: 3.2
  parse_max_time_step: &tmts 200
  decode_max_time_step: *smts
  cuda: true
  clip: 5.0
  eval_bs: 10
  greedy: true
  linear_warmup: false
  warm_up: 5000
  epoch: 5
  patience: 25
  # seq2seq model parameter
  embed_size: &ves 300
  hidden_size: &vhs 300
  num_layers: &vnl 1
  rnn_drop: &vrd 0.0
  rnn_type: gru
  enc_embed_dim: *ves
  tree_embed_dim: 300
  enc_hidden_dim: *vhs
  enc_num_layers: *vnl
  bidirectional: true
  mapper_type: mapping
  dec_embed_dim: *ves
  dec_hidden_dim: *vhs
  dec_num_layers: *vnl
  # vae parameter
  latent_size: 300
  sample_size: 5
  share_embed: true
  enc_ed: 0.0
  enc_rd: 0.0
  dec_ed: *vrd
  dec_rd: *vrd
  unk_rate: 0.5
  k: 0.0025
  x0: 2500
  anneal_function: sigmoid
  unk_schedule: fixed
  src_wd: false
  tgt_wd: true
  peak_anneal: false
  init_step_kl_weight: ~
  stop_clip_kl: 0.3
  kl_sem: 1.5
  #  aux_weight: ~
  bow_factor: 0.5
  mul_sem: 5.0
  content_factor: 0.0
  report: ~